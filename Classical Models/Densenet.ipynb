{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Densenet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kpa4EySxGelN",
        "colab_type": "code",
        "outputId": "cfbbb184-cc67-45a0-d4ad-598695f56b5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "!pip3 install setproctitle\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting setproctitle\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/0d/dc0d2234aacba6cf1a729964383e3452c52096dc695581248b548786f2b3/setproctitle-1.1.10.tar.gz\n",
            "Building wheels for collected packages: setproctitle\n",
            "  Building wheel for setproctitle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for setproctitle: filename=setproctitle-1.1.10-cp36-cp36m-linux_x86_64.whl size=33925 sha256=93ef33ce4296345c3ced8ff94cfb9abe6bd8bd7afe9efb2016e6d042709d2f58\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/b1/a6/9719530228e258eba904501fef99d5d85c80d52bd8f14438a3\n",
            "Successfully built setproctitle\n",
            "Installing collected packages: setproctitle\n",
            "Successfully installed setproctitle-1.1.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5cPKkZrPdQX",
        "colab_type": "code",
        "outputId": "02e7a984-c87e-41ff-be31-4fc330810f78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmJdthyxGevJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch.nn.functional as F\n",
        "# from torch.autograd import Variable\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "import sys\n",
        "import math\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, nChannels, growthRate):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        interChannels = 4*growthRate\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n",
        "                               bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(interChannels)\n",
        "        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n",
        "                               padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out = torch.cat((x, out), 1)\n",
        "        return out\n",
        "\n",
        "class SingleLayer(nn.Module):\n",
        "    def __init__(self, nChannels, growthRate):\n",
        "        super(SingleLayer, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n",
        "                               padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = torch.cat((x, out), 1)\n",
        "        return out\n",
        "\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, nChannels, nOutChannels):\n",
        "        super(Transition, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n",
        "                               bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = F.avg_pool2d(out, 2)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, growthRate, depth, reduction, nClasses, bottleneck):\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        nDenseBlocks = (depth-4) // 3\n",
        "        if bottleneck:\n",
        "            nDenseBlocks //= 2\n",
        "\n",
        "        nChannels = 2*growthRate\n",
        "        self.conv1 = nn.Conv2d(3, nChannels, kernel_size=3, padding=1,\n",
        "                               bias=False)\n",
        "        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
        "        nChannels += nDenseBlocks*growthRate\n",
        "        nOutChannels = int(math.floor(nChannels*reduction))\n",
        "        self.trans1 = Transition(nChannels, nOutChannels)\n",
        "\n",
        "        nChannels = nOutChannels\n",
        "        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
        "        nChannels += nDenseBlocks*growthRate\n",
        "        nOutChannels = int(math.floor(nChannels*reduction))\n",
        "        self.trans2 = Transition(nChannels, nOutChannels)\n",
        "\n",
        "        nChannels = nOutChannels\n",
        "        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
        "        nChannels += nDenseBlocks*growthRate\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "        self.fc = nn.Linear(nChannels, nClasses)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
        "        layers = []\n",
        "        for i in range(int(nDenseBlocks)):\n",
        "            if bottleneck:\n",
        "                layers.append(Bottleneck(nChannels, growthRate))\n",
        "            else:\n",
        "                layers.append(SingleLayer(nChannels, growthRate))\n",
        "            nChannels += growthRate\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.trans1(self.dense1(out))\n",
        "        out = self.trans2(self.dense2(out))\n",
        "        out = self.dense3(out)\n",
        "        out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 8))\n",
        "        out = F.log_softmax(self.fc(out))\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03wJKv_0HABG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From https://gist.github.com/apaszke/01aae7a0494c55af6242f06fad1f8b70\n",
        "from graphviz import Digraph\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def save(fname, creator):\n",
        "    dot = Digraph(comment='LRP',\n",
        "                node_attr={'style': 'filled', 'shape': 'box'})\n",
        "    #, 'fillcolor': 'lightblue'})\n",
        "\n",
        "    seen = set()\n",
        "\n",
        "    def add_nodes(var):\n",
        "        if var not in seen:\n",
        "            if isinstance(var, Variable):\n",
        "                dot.node(str(id(var)), str(var.size()), fillcolor='lightblue')\n",
        "            else:\n",
        "                dot.node(str(id(var)), type(var).__name__)\n",
        "            seen.add(var)\n",
        "            if hasattr(var, 'previous_functions'):\n",
        "                for u in var.previous_functions:\n",
        "                    dot.edge(str(id(u[0])), str(id(var)))\n",
        "                    add_nodes(u[0])\n",
        "\n",
        "    add_nodes(creator)\n",
        "    dot.save(fname)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2rRyvwoGe5Q",
        "colab_type": "code",
        "outputId": "6dd19684-e1ce-4c50-8405-2383307dd7c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "# import argparse\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch.nn.functional as F\n",
        "# from torch.autograd import Variable\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "\n",
        "import shutil\n",
        "\n",
        "import setproctitle\n",
        "\n",
        "# import densenet\n",
        "# import make_graph\n",
        "\n",
        "def main():\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument('--batchSz', type=int, default=64)\n",
        "    # parser.add_argument('--nEpochs', type=int, default=300)\n",
        "    # parser.add_argument('--no-cuda', action='store_true')\n",
        "    # parser.add_argument('--save')\n",
        "    # parser.add_argument('--seed', type=int, default=1)\n",
        "    # parser.add_argument('--opt', type=str, default='sgd',\n",
        "    #                     choices=('sgd', 'adam', 'rmsprop'))\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    # args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "    # args.save = args.save or 'work/densenet.base'\n",
        "    # setproctitle.setproctitle(args.save)\n",
        "    \n",
        "    # torch.manual_seed(args.seed)\n",
        "    # if args.cuda:\n",
        "    #     torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "    # if os.path.exists(args.save):\n",
        "    #     shutil.rmtree(args.save)\n",
        "    # os.makedirs(args.save, exist_ok=True)\n",
        "\n",
        "    normMean = [0.49139968, 0.48215827, 0.44653124]\n",
        "    normStd = [0.24703233, 0.24348505, 0.26158768]\n",
        "    normTransform = transforms.Normalize(normMean, normStd)\n",
        "\n",
        "    trainTransform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        normTransform\n",
        "    ])\n",
        "    testTransform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normTransform\n",
        "    ])\n",
        "    device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
        "    # kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
        "    trainLoader = DataLoader(\n",
        "        dset.CIFAR10(root='cifar', train=True, download=True,\n",
        "                     transform=trainTransform),\n",
        "        batch_size=64, shuffle=True, num_workers=4)\n",
        "    testLoader = DataLoader(\n",
        "        dset.CIFAR10(root='cifar', train=False, download=True,\n",
        "                     transform=testTransform),\n",
        "        batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    net = DenseNet(growthRate=12, depth=100, reduction=0.5,\n",
        "                            bottleneck=True, nClasses=10)\n",
        "\n",
        "    print('  + Number of params: {}'.format(\n",
        "        sum([p.data.nelement() for p in net.parameters()])))\n",
        "    # if args.cuda:\n",
        "    net = net.to(device)\n",
        "\n",
        "    # if args.opt == 'sgd':\n",
        "    # optimizer = optim.SGD(net.parameters(), lr=1e-1,\n",
        "                            # momentum=0.9, weight_decay=1e-4)\n",
        "    # elif args.opt == 'adam':\n",
        "    optimizer = optim.Adam(net.parameters(), weight_decay=1e-4)\n",
        "    # elif args.opt == 'rmsprop':\n",
        "    #     optimizer = optim.RMSprop(net.parameters(), weight_decay=1e-4)\n",
        "\n",
        "    # trainF = open(os.path.join(args.save, 'train.csv'), 'w')\n",
        "    # testF = open(os.path.join(args.save, 'test.csv'), 'w')\n",
        "\n",
        "    for epoch in range(1, 300 + 1):\n",
        "        # adjust_opt('sgd', optimizer, epoch)\n",
        "        train(epoch, net, trainLoader, optimizer,device)\n",
        "        test(epoch, net, testLoader, optimizer,device)\n",
        "        # torch.save(net, os.path.join(args.save, 'latest.pth'))\n",
        "        # os.system('./plot.py {} &'.format(args.save))\n",
        "\n",
        "    # trainF.close()\n",
        "    # testF.close()\n",
        "\n",
        "def train(epoch, net, trainLoader, optimizer,device):\n",
        "    net.train()\n",
        "    nProcessed = 0\n",
        "    nTrain = len(trainLoader.dataset)\n",
        "    for batch_idx, (data, target) in enumerate(trainLoader):\n",
        "        # if args.cuda:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # data, target = Variable(data), Variable(target)\n",
        "        optimizer.zero_grad()\n",
        "        output = net(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        # make_graph.save('/tmp/t.dot', loss.creator); assert(False)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        nProcessed += len(data)\n",
        "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
        "        incorrect = pred.ne(target.data).cpu().sum()\n",
        "        err = 100.*incorrect/len(data)\n",
        "        partialEpoch = epoch + batch_idx / len(trainLoader) - 1\n",
        "        print('Train Epoch: {:.2f} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tError: {:.6f}'.format(\n",
        "            partialEpoch, nProcessed, nTrain, 100. * batch_idx / len(trainLoader),\n",
        "            loss.item(), err))\n",
        "\n",
        "        # trainF.write('{},{},{}\\n'.format(partialEpoch, loss.data[0], err))\n",
        "        # trainF.flush()\n",
        "\n",
        "def test(epoch, net, testLoader, optimizer,device):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    incorrect = 0\n",
        "    for data, target in testLoader:\n",
        "        # if args.cuda:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = net(data)\n",
        "        test_loss += F.nll_loss(output, target).item()\n",
        "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
        "        incorrect += pred.ne(target.data).cpu().sum()\n",
        "\n",
        "    test_loss = test_loss\n",
        "    test_loss /= len(testLoader) # loss function already averages over batch size\n",
        "    nTotal = len(testLoader.dataset)\n",
        "    err = 100.*incorrect/nTotal\n",
        "    print('\\nTest set: Average loss: {:.4f}, Error: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, incorrect, nTotal, err))\n",
        "\n",
        "    # testF.write('{},{},{}\\n'.format(epoch, test_loss, err))\n",
        "    # testF.flush()\n",
        "\n",
        "def adjust_opt(optAlg, optimizer, epoch):\n",
        "    if optAlg == 'sgd':\n",
        "        if epoch < 150: lr = 1e-1\n",
        "        elif epoch == 150: lr = 1e-2\n",
        "        elif epoch == 225: lr = 1e-3\n",
        "        else: return\n",
        "\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "# if __name__=='__main__':\n",
        "# device = torch.device\n",
        "main()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.3667, Error: 1189/10000 (12%)\n",
            "\n",
            "Train Epoch: 23.00 [64/50000 (0%)]\tLoss: 0.128795\tError: 6.250000\n",
            "Train Epoch: 23.00 [128/50000 (0%)]\tLoss: 0.309722\tError: 12.500000\n",
            "Train Epoch: 23.00 [192/50000 (0%)]\tLoss: 0.197141\tError: 7.812500\n",
            "Train Epoch: 23.00 [256/50000 (0%)]\tLoss: 0.177331\tError: 9.375000\n",
            "Train Epoch: 23.01 [320/50000 (1%)]\tLoss: 0.192000\tError: 6.250000\n",
            "Train Epoch: 23.01 [384/50000 (1%)]\tLoss: 0.269335\tError: 9.375000\n",
            "Train Epoch: 23.01 [448/50000 (1%)]\tLoss: 0.341553\tError: 14.062500\n",
            "Train Epoch: 23.01 [512/50000 (1%)]\tLoss: 0.212444\tError: 10.937500\n",
            "Train Epoch: 23.01 [576/50000 (1%)]\tLoss: 0.249649\tError: 4.687500\n",
            "Train Epoch: 23.01 [640/50000 (1%)]\tLoss: 0.238301\tError: 7.812500\n",
            "Train Epoch: 23.01 [704/50000 (1%)]\tLoss: 0.329480\tError: 10.937500\n",
            "Train Epoch: 23.01 [768/50000 (1%)]\tLoss: 0.191726\tError: 6.250000\n",
            "Train Epoch: 23.02 [832/50000 (2%)]\tLoss: 0.114555\tError: 3.125000\n",
            "Train Epoch: 23.02 [896/50000 (2%)]\tLoss: 0.149550\tError: 6.250000\n",
            "Train Epoch: 23.02 [960/50000 (2%)]\tLoss: 0.152437\tError: 3.125000\n",
            "Train Epoch: 23.02 [1024/50000 (2%)]\tLoss: 0.343054\tError: 12.500000\n",
            "Train Epoch: 23.02 [1088/50000 (2%)]\tLoss: 0.283911\tError: 6.250000\n",
            "Train Epoch: 23.02 [1152/50000 (2%)]\tLoss: 0.119745\tError: 3.125000\n",
            "Train Epoch: 23.02 [1216/50000 (2%)]\tLoss: 0.151752\tError: 6.250000\n",
            "Train Epoch: 23.02 [1280/50000 (2%)]\tLoss: 0.346935\tError: 10.937500\n",
            "Train Epoch: 23.03 [1344/50000 (3%)]\tLoss: 0.170842\tError: 4.687500\n",
            "Train Epoch: 23.03 [1408/50000 (3%)]\tLoss: 0.324426\tError: 9.375000\n",
            "Train Epoch: 23.03 [1472/50000 (3%)]\tLoss: 0.161715\tError: 7.812500\n",
            "Train Epoch: 23.03 [1536/50000 (3%)]\tLoss: 0.132063\tError: 4.687500\n",
            "Train Epoch: 23.03 [1600/50000 (3%)]\tLoss: 0.326027\tError: 14.062500\n",
            "Train Epoch: 23.03 [1664/50000 (3%)]\tLoss: 0.200425\tError: 6.250000\n",
            "Train Epoch: 23.03 [1728/50000 (3%)]\tLoss: 0.094800\tError: 1.562500\n",
            "Train Epoch: 23.03 [1792/50000 (3%)]\tLoss: 0.257015\tError: 10.937500\n",
            "Train Epoch: 23.04 [1856/50000 (4%)]\tLoss: 0.405056\tError: 14.062500\n",
            "Train Epoch: 23.04 [1920/50000 (4%)]\tLoss: 0.269897\tError: 7.812500\n",
            "Train Epoch: 23.04 [1984/50000 (4%)]\tLoss: 0.293815\tError: 7.812500\n",
            "Train Epoch: 23.04 [2048/50000 (4%)]\tLoss: 0.106191\tError: 1.562500\n",
            "Train Epoch: 23.04 [2112/50000 (4%)]\tLoss: 0.225244\tError: 6.250000\n",
            "Train Epoch: 23.04 [2176/50000 (4%)]\tLoss: 0.163246\tError: 7.812500\n",
            "Train Epoch: 23.04 [2240/50000 (4%)]\tLoss: 0.149362\tError: 3.125000\n",
            "Train Epoch: 23.04 [2304/50000 (4%)]\tLoss: 0.172258\tError: 6.250000\n",
            "Train Epoch: 23.05 [2368/50000 (5%)]\tLoss: 0.221456\tError: 4.687500\n",
            "Train Epoch: 23.05 [2432/50000 (5%)]\tLoss: 0.300763\tError: 9.375000\n",
            "Train Epoch: 23.05 [2496/50000 (5%)]\tLoss: 0.313100\tError: 10.937500\n",
            "Train Epoch: 23.05 [2560/50000 (5%)]\tLoss: 0.093844\tError: 0.000000\n",
            "Train Epoch: 23.05 [2624/50000 (5%)]\tLoss: 0.100236\tError: 1.562500\n",
            "Train Epoch: 23.05 [2688/50000 (5%)]\tLoss: 0.094814\tError: 3.125000\n",
            "Train Epoch: 23.05 [2752/50000 (5%)]\tLoss: 0.187513\tError: 6.250000\n",
            "Train Epoch: 23.05 [2816/50000 (5%)]\tLoss: 0.107345\tError: 1.562500\n",
            "Train Epoch: 23.06 [2880/50000 (6%)]\tLoss: 0.172093\tError: 6.250000\n",
            "Train Epoch: 23.06 [2944/50000 (6%)]\tLoss: 0.284668\tError: 9.375000\n",
            "Train Epoch: 23.06 [3008/50000 (6%)]\tLoss: 0.180968\tError: 6.250000\n",
            "Train Epoch: 23.06 [3072/50000 (6%)]\tLoss: 0.196848\tError: 6.250000\n",
            "Train Epoch: 23.06 [3136/50000 (6%)]\tLoss: 0.261065\tError: 7.812500\n",
            "Train Epoch: 23.06 [3200/50000 (6%)]\tLoss: 0.204648\tError: 7.812500\n",
            "Train Epoch: 23.06 [3264/50000 (6%)]\tLoss: 0.157774\tError: 3.125000\n",
            "Train Epoch: 23.07 [3328/50000 (7%)]\tLoss: 0.261298\tError: 9.375000\n",
            "Train Epoch: 23.07 [3392/50000 (7%)]\tLoss: 0.198762\tError: 9.375000\n",
            "Train Epoch: 23.07 [3456/50000 (7%)]\tLoss: 0.089295\tError: 3.125000\n",
            "Train Epoch: 23.07 [3520/50000 (7%)]\tLoss: 0.360239\tError: 12.500000\n",
            "Train Epoch: 23.07 [3584/50000 (7%)]\tLoss: 0.109729\tError: 3.125000\n",
            "Train Epoch: 23.07 [3648/50000 (7%)]\tLoss: 0.216453\tError: 12.500000\n",
            "Train Epoch: 23.07 [3712/50000 (7%)]\tLoss: 0.105193\tError: 3.125000\n",
            "Train Epoch: 23.07 [3776/50000 (7%)]\tLoss: 0.252247\tError: 12.500000\n",
            "Train Epoch: 23.08 [3840/50000 (8%)]\tLoss: 0.151790\tError: 7.812500\n",
            "Train Epoch: 23.08 [3904/50000 (8%)]\tLoss: 0.248812\tError: 7.812500\n",
            "Train Epoch: 23.08 [3968/50000 (8%)]\tLoss: 0.222987\tError: 6.250000\n",
            "Train Epoch: 23.08 [4032/50000 (8%)]\tLoss: 0.174307\tError: 6.250000\n",
            "Train Epoch: 23.08 [4096/50000 (8%)]\tLoss: 0.257599\tError: 12.500000\n",
            "Train Epoch: 23.08 [4160/50000 (8%)]\tLoss: 0.206585\tError: 10.937500\n",
            "Train Epoch: 23.08 [4224/50000 (8%)]\tLoss: 0.200576\tError: 7.812500\n",
            "Train Epoch: 23.08 [4288/50000 (8%)]\tLoss: 0.253851\tError: 12.500000\n",
            "Train Epoch: 23.09 [4352/50000 (9%)]\tLoss: 0.186830\tError: 4.687500\n",
            "Train Epoch: 23.09 [4416/50000 (9%)]\tLoss: 0.105361\tError: 1.562500\n",
            "Train Epoch: 23.09 [4480/50000 (9%)]\tLoss: 0.188617\tError: 4.687500\n",
            "Train Epoch: 23.09 [4544/50000 (9%)]\tLoss: 0.200149\tError: 10.937500\n",
            "Train Epoch: 23.09 [4608/50000 (9%)]\tLoss: 0.147099\tError: 7.812500\n",
            "Train Epoch: 23.09 [4672/50000 (9%)]\tLoss: 0.110619\tError: 4.687500\n",
            "Train Epoch: 23.09 [4736/50000 (9%)]\tLoss: 0.239629\tError: 7.812500\n",
            "Train Epoch: 23.09 [4800/50000 (9%)]\tLoss: 0.139670\tError: 3.125000\n",
            "Train Epoch: 23.10 [4864/50000 (10%)]\tLoss: 0.194327\tError: 1.562500\n",
            "Train Epoch: 23.10 [4928/50000 (10%)]\tLoss: 0.193854\tError: 6.250000\n",
            "Train Epoch: 23.10 [4992/50000 (10%)]\tLoss: 0.371813\tError: 10.937500\n",
            "Train Epoch: 23.10 [5056/50000 (10%)]\tLoss: 0.345806\tError: 12.500000\n",
            "Train Epoch: 23.10 [5120/50000 (10%)]\tLoss: 0.151378\tError: 7.812500\n",
            "Train Epoch: 23.10 [5184/50000 (10%)]\tLoss: 0.187606\tError: 4.687500\n",
            "Train Epoch: 23.10 [5248/50000 (10%)]\tLoss: 0.088218\tError: 1.562500\n",
            "Train Epoch: 23.10 [5312/50000 (10%)]\tLoss: 0.346296\tError: 12.500000\n",
            "Train Epoch: 23.11 [5376/50000 (11%)]\tLoss: 0.164613\tError: 4.687500\n",
            "Train Epoch: 23.11 [5440/50000 (11%)]\tLoss: 0.352970\tError: 15.625000\n",
            "Train Epoch: 23.11 [5504/50000 (11%)]\tLoss: 0.240484\tError: 7.812500\n",
            "Train Epoch: 23.11 [5568/50000 (11%)]\tLoss: 0.345137\tError: 15.625000\n",
            "Train Epoch: 23.11 [5632/50000 (11%)]\tLoss: 0.181857\tError: 6.250000\n",
            "Train Epoch: 23.11 [5696/50000 (11%)]\tLoss: 0.193726\tError: 6.250000\n",
            "Train Epoch: 23.11 [5760/50000 (11%)]\tLoss: 0.361777\tError: 9.375000\n",
            "Train Epoch: 23.12 [5824/50000 (12%)]\tLoss: 0.216093\tError: 6.250000\n",
            "Train Epoch: 23.12 [5888/50000 (12%)]\tLoss: 0.191677\tError: 9.375000\n",
            "Train Epoch: 23.12 [5952/50000 (12%)]\tLoss: 0.205017\tError: 9.375000\n",
            "Train Epoch: 23.12 [6016/50000 (12%)]\tLoss: 0.160771\tError: 7.812500\n",
            "Train Epoch: 23.12 [6080/50000 (12%)]\tLoss: 0.247062\tError: 9.375000\n",
            "Train Epoch: 23.12 [6144/50000 (12%)]\tLoss: 0.199347\tError: 7.812500\n",
            "Train Epoch: 23.12 [6208/50000 (12%)]\tLoss: 0.179342\tError: 4.687500\n",
            "Train Epoch: 23.12 [6272/50000 (12%)]\tLoss: 0.188842\tError: 7.812500\n",
            "Train Epoch: 23.13 [6336/50000 (13%)]\tLoss: 0.340779\tError: 7.812500\n",
            "Train Epoch: 23.13 [6400/50000 (13%)]\tLoss: 0.192765\tError: 7.812500\n",
            "Train Epoch: 23.13 [6464/50000 (13%)]\tLoss: 0.208249\tError: 9.375000\n",
            "Train Epoch: 23.13 [6528/50000 (13%)]\tLoss: 0.161326\tError: 4.687500\n",
            "Train Epoch: 23.13 [6592/50000 (13%)]\tLoss: 0.237302\tError: 7.812500\n",
            "Train Epoch: 23.13 [6656/50000 (13%)]\tLoss: 0.171796\tError: 9.375000\n",
            "Train Epoch: 23.13 [6720/50000 (13%)]\tLoss: 0.113006\tError: 7.812500\n",
            "Train Epoch: 23.13 [6784/50000 (13%)]\tLoss: 0.180480\tError: 9.375000\n",
            "Train Epoch: 23.14 [6848/50000 (14%)]\tLoss: 0.198621\tError: 3.125000\n",
            "Train Epoch: 23.14 [6912/50000 (14%)]\tLoss: 0.238068\tError: 10.937500\n",
            "Train Epoch: 23.14 [6976/50000 (14%)]\tLoss: 0.245908\tError: 6.250000\n",
            "Train Epoch: 23.14 [7040/50000 (14%)]\tLoss: 0.135273\tError: 3.125000\n",
            "Train Epoch: 23.14 [7104/50000 (14%)]\tLoss: 0.135463\tError: 3.125000\n",
            "Train Epoch: 23.14 [7168/50000 (14%)]\tLoss: 0.334646\tError: 14.062500\n",
            "Train Epoch: 23.14 [7232/50000 (14%)]\tLoss: 0.172363\tError: 9.375000\n",
            "Train Epoch: 23.14 [7296/50000 (14%)]\tLoss: 0.231356\tError: 7.812500\n",
            "Train Epoch: 23.15 [7360/50000 (15%)]\tLoss: 0.254169\tError: 4.687500\n",
            "Train Epoch: 23.15 [7424/50000 (15%)]\tLoss: 0.261812\tError: 7.812500\n",
            "Train Epoch: 23.15 [7488/50000 (15%)]\tLoss: 0.161247\tError: 6.250000\n",
            "Train Epoch: 23.15 [7552/50000 (15%)]\tLoss: 0.181140\tError: 6.250000\n",
            "Train Epoch: 23.15 [7616/50000 (15%)]\tLoss: 0.228304\tError: 6.250000\n",
            "Train Epoch: 23.15 [7680/50000 (15%)]\tLoss: 0.330533\tError: 10.937500\n",
            "Train Epoch: 23.15 [7744/50000 (15%)]\tLoss: 0.170171\tError: 4.687500\n",
            "Train Epoch: 23.15 [7808/50000 (15%)]\tLoss: 0.117002\tError: 4.687500\n",
            "Train Epoch: 23.16 [7872/50000 (16%)]\tLoss: 0.143950\tError: 4.687500\n",
            "Train Epoch: 23.16 [7936/50000 (16%)]\tLoss: 0.350149\tError: 9.375000\n",
            "Train Epoch: 23.16 [8000/50000 (16%)]\tLoss: 0.244218\tError: 10.937500\n",
            "Train Epoch: 23.16 [8064/50000 (16%)]\tLoss: 0.195165\tError: 6.250000\n",
            "Train Epoch: 23.16 [8128/50000 (16%)]\tLoss: 0.180430\tError: 6.250000\n",
            "Train Epoch: 23.16 [8192/50000 (16%)]\tLoss: 0.342467\tError: 12.500000\n",
            "Train Epoch: 23.16 [8256/50000 (16%)]\tLoss: 0.197256\tError: 7.812500\n",
            "Train Epoch: 23.16 [8320/50000 (16%)]\tLoss: 0.155189\tError: 3.125000\n",
            "Train Epoch: 23.17 [8384/50000 (17%)]\tLoss: 0.170924\tError: 4.687500\n",
            "Train Epoch: 23.17 [8448/50000 (17%)]\tLoss: 0.342781\tError: 14.062500\n",
            "Train Epoch: 23.17 [8512/50000 (17%)]\tLoss: 0.230975\tError: 10.937500\n",
            "Train Epoch: 23.17 [8576/50000 (17%)]\tLoss: 0.236425\tError: 6.250000\n",
            "Train Epoch: 23.17 [8640/50000 (17%)]\tLoss: 0.106609\tError: 4.687500\n",
            "Train Epoch: 23.17 [8704/50000 (17%)]\tLoss: 0.153118\tError: 4.687500\n",
            "Train Epoch: 23.17 [8768/50000 (17%)]\tLoss: 0.230932\tError: 7.812500\n",
            "Train Epoch: 23.18 [8832/50000 (18%)]\tLoss: 0.228401\tError: 7.812500\n",
            "Train Epoch: 23.18 [8896/50000 (18%)]\tLoss: 0.241095\tError: 7.812500\n",
            "Train Epoch: 23.18 [8960/50000 (18%)]\tLoss: 0.236941\tError: 7.812500\n",
            "Train Epoch: 23.18 [9024/50000 (18%)]\tLoss: 0.124717\tError: 1.562500\n",
            "Train Epoch: 23.18 [9088/50000 (18%)]\tLoss: 0.183925\tError: 4.687500\n",
            "Train Epoch: 23.18 [9152/50000 (18%)]\tLoss: 0.121310\tError: 3.125000\n",
            "Train Epoch: 23.18 [9216/50000 (18%)]\tLoss: 0.283158\tError: 12.500000\n",
            "Train Epoch: 23.18 [9280/50000 (18%)]\tLoss: 0.130583\tError: 4.687500\n",
            "Train Epoch: 23.19 [9344/50000 (19%)]\tLoss: 0.179887\tError: 6.250000\n",
            "Train Epoch: 23.19 [9408/50000 (19%)]\tLoss: 0.096427\tError: 4.687500\n",
            "Train Epoch: 23.19 [9472/50000 (19%)]\tLoss: 0.212981\tError: 6.250000\n",
            "Train Epoch: 23.19 [9536/50000 (19%)]\tLoss: 0.357101\tError: 10.937500\n",
            "Train Epoch: 23.19 [9600/50000 (19%)]\tLoss: 0.249852\tError: 9.375000\n",
            "Train Epoch: 23.19 [9664/50000 (19%)]\tLoss: 0.221045\tError: 9.375000\n",
            "Train Epoch: 23.19 [9728/50000 (19%)]\tLoss: 0.304502\tError: 7.812500\n",
            "Train Epoch: 23.19 [9792/50000 (19%)]\tLoss: 0.188632\tError: 9.375000\n",
            "Train Epoch: 23.20 [9856/50000 (20%)]\tLoss: 0.168160\tError: 4.687500\n",
            "Train Epoch: 23.20 [9920/50000 (20%)]\tLoss: 0.193690\tError: 7.812500\n",
            "Train Epoch: 23.20 [9984/50000 (20%)]\tLoss: 0.191628\tError: 6.250000\n",
            "Train Epoch: 23.20 [10048/50000 (20%)]\tLoss: 0.286308\tError: 12.500000\n",
            "Train Epoch: 23.20 [10112/50000 (20%)]\tLoss: 0.325427\tError: 12.500000\n",
            "Train Epoch: 23.20 [10176/50000 (20%)]\tLoss: 0.271931\tError: 7.812500\n",
            "Train Epoch: 23.20 [10240/50000 (20%)]\tLoss: 0.361011\tError: 12.500000\n",
            "Train Epoch: 23.20 [10304/50000 (20%)]\tLoss: 0.263335\tError: 6.250000\n",
            "Train Epoch: 23.21 [10368/50000 (21%)]\tLoss: 0.374002\tError: 10.937500\n",
            "Train Epoch: 23.21 [10432/50000 (21%)]\tLoss: 0.329407\tError: 9.375000\n",
            "Train Epoch: 23.21 [10496/50000 (21%)]\tLoss: 0.119621\tError: 4.687500\n",
            "Train Epoch: 23.21 [10560/50000 (21%)]\tLoss: 0.187151\tError: 7.812500\n",
            "Train Epoch: 23.21 [10624/50000 (21%)]\tLoss: 0.275436\tError: 10.937500\n",
            "Train Epoch: 23.21 [10688/50000 (21%)]\tLoss: 0.407402\tError: 12.500000\n",
            "Train Epoch: 23.21 [10752/50000 (21%)]\tLoss: 0.204973\tError: 6.250000\n",
            "Train Epoch: 23.21 [10816/50000 (21%)]\tLoss: 0.171605\tError: 3.125000\n",
            "Train Epoch: 23.22 [10880/50000 (22%)]\tLoss: 0.186890\tError: 6.250000\n",
            "Train Epoch: 23.22 [10944/50000 (22%)]\tLoss: 0.185642\tError: 7.812500\n",
            "Train Epoch: 23.22 [11008/50000 (22%)]\tLoss: 0.425534\tError: 12.500000\n",
            "Train Epoch: 23.22 [11072/50000 (22%)]\tLoss: 0.270551\tError: 7.812500\n",
            "Train Epoch: 23.22 [11136/50000 (22%)]\tLoss: 0.207757\tError: 7.812500\n",
            "Train Epoch: 23.22 [11200/50000 (22%)]\tLoss: 0.213775\tError: 7.812500\n",
            "Train Epoch: 23.22 [11264/50000 (22%)]\tLoss: 0.194849\tError: 6.250000\n",
            "Train Epoch: 23.23 [11328/50000 (23%)]\tLoss: 0.189523\tError: 9.375000\n",
            "Train Epoch: 23.23 [11392/50000 (23%)]\tLoss: 0.320996\tError: 14.062500\n",
            "Train Epoch: 23.23 [11456/50000 (23%)]\tLoss: 0.238761\tError: 7.812500\n",
            "Train Epoch: 23.23 [11520/50000 (23%)]\tLoss: 0.109947\tError: 4.687500\n",
            "Train Epoch: 23.23 [11584/50000 (23%)]\tLoss: 0.098409\tError: 1.562500\n",
            "Train Epoch: 23.23 [11648/50000 (23%)]\tLoss: 0.202794\tError: 7.812500\n",
            "Train Epoch: 23.23 [11712/50000 (23%)]\tLoss: 0.159224\tError: 4.687500\n",
            "Train Epoch: 23.23 [11776/50000 (23%)]\tLoss: 0.200415\tError: 4.687500\n",
            "Train Epoch: 23.24 [11840/50000 (24%)]\tLoss: 0.258689\tError: 7.812500\n",
            "Train Epoch: 23.24 [11904/50000 (24%)]\tLoss: 0.226132\tError: 7.812500\n",
            "Train Epoch: 23.24 [11968/50000 (24%)]\tLoss: 0.198777\tError: 4.687500\n",
            "Train Epoch: 23.24 [12032/50000 (24%)]\tLoss: 0.172246\tError: 6.250000\n",
            "Train Epoch: 23.24 [12096/50000 (24%)]\tLoss: 0.162640\tError: 7.812500\n",
            "Train Epoch: 23.24 [12160/50000 (24%)]\tLoss: 0.182031\tError: 7.812500\n",
            "Train Epoch: 23.24 [12224/50000 (24%)]\tLoss: 0.238851\tError: 4.687500\n",
            "Train Epoch: 23.24 [12288/50000 (24%)]\tLoss: 0.156901\tError: 3.125000\n",
            "Train Epoch: 23.25 [12352/50000 (25%)]\tLoss: 0.182010\tError: 6.250000\n",
            "Train Epoch: 23.25 [12416/50000 (25%)]\tLoss: 0.273458\tError: 10.937500\n",
            "Train Epoch: 23.25 [12480/50000 (25%)]\tLoss: 0.245219\tError: 7.812500\n",
            "Train Epoch: 23.25 [12544/50000 (25%)]\tLoss: 0.102121\tError: 1.562500\n",
            "Train Epoch: 23.25 [12608/50000 (25%)]\tLoss: 0.191376\tError: 3.125000\n",
            "Train Epoch: 23.25 [12672/50000 (25%)]\tLoss: 0.275289\tError: 9.375000\n",
            "Train Epoch: 23.25 [12736/50000 (25%)]\tLoss: 0.210086\tError: 7.812500\n",
            "Train Epoch: 23.25 [12800/50000 (25%)]\tLoss: 0.170279\tError: 6.250000\n",
            "Train Epoch: 23.26 [12864/50000 (26%)]\tLoss: 0.242306\tError: 9.375000\n",
            "Train Epoch: 23.26 [12928/50000 (26%)]\tLoss: 0.276117\tError: 14.062500\n",
            "Train Epoch: 23.26 [12992/50000 (26%)]\tLoss: 0.167865\tError: 6.250000\n",
            "Train Epoch: 23.26 [13056/50000 (26%)]\tLoss: 0.215625\tError: 7.812500\n",
            "Train Epoch: 23.26 [13120/50000 (26%)]\tLoss: 0.437077\tError: 14.062500\n",
            "Train Epoch: 23.26 [13184/50000 (26%)]\tLoss: 0.263256\tError: 7.812500\n",
            "Train Epoch: 23.26 [13248/50000 (26%)]\tLoss: 0.274852\tError: 12.500000\n",
            "Train Epoch: 23.26 [13312/50000 (26%)]\tLoss: 0.299474\tError: 14.062500\n",
            "Train Epoch: 23.27 [13376/50000 (27%)]\tLoss: 0.217520\tError: 7.812500\n",
            "Train Epoch: 23.27 [13440/50000 (27%)]\tLoss: 0.194996\tError: 4.687500\n",
            "Train Epoch: 23.27 [13504/50000 (27%)]\tLoss: 0.422375\tError: 10.937500\n",
            "Train Epoch: 23.27 [13568/50000 (27%)]\tLoss: 0.108132\tError: 3.125000\n",
            "Train Epoch: 23.27 [13632/50000 (27%)]\tLoss: 0.193382\tError: 7.812500\n",
            "Train Epoch: 23.27 [13696/50000 (27%)]\tLoss: 0.210966\tError: 7.812500\n",
            "Train Epoch: 23.27 [13760/50000 (27%)]\tLoss: 0.124501\tError: 1.562500\n",
            "Train Epoch: 23.27 [13824/50000 (27%)]\tLoss: 0.156028\tError: 7.812500\n",
            "Train Epoch: 23.28 [13888/50000 (28%)]\tLoss: 0.303843\tError: 10.937500\n",
            "Train Epoch: 23.28 [13952/50000 (28%)]\tLoss: 0.220986\tError: 7.812500\n",
            "Train Epoch: 23.28 [14016/50000 (28%)]\tLoss: 0.168131\tError: 7.812500\n",
            "Train Epoch: 23.28 [14080/50000 (28%)]\tLoss: 0.182209\tError: 7.812500\n",
            "Train Epoch: 23.28 [14144/50000 (28%)]\tLoss: 0.413976\tError: 14.062500\n",
            "Train Epoch: 23.28 [14208/50000 (28%)]\tLoss: 0.161247\tError: 7.812500\n",
            "Train Epoch: 23.28 [14272/50000 (28%)]\tLoss: 0.147409\tError: 6.250000\n",
            "Train Epoch: 23.29 [14336/50000 (29%)]\tLoss: 0.159179\tError: 3.125000\n",
            "Train Epoch: 23.29 [14400/50000 (29%)]\tLoss: 0.193783\tError: 6.250000\n",
            "Train Epoch: 23.29 [14464/50000 (29%)]\tLoss: 0.263079\tError: 7.812500\n",
            "Train Epoch: 23.29 [14528/50000 (29%)]\tLoss: 0.147212\tError: 6.250000\n",
            "Train Epoch: 23.29 [14592/50000 (29%)]\tLoss: 0.220033\tError: 6.250000\n",
            "Train Epoch: 23.29 [14656/50000 (29%)]\tLoss: 0.273088\tError: 7.812500\n",
            "Train Epoch: 23.29 [14720/50000 (29%)]\tLoss: 0.162266\tError: 6.250000\n",
            "Train Epoch: 23.29 [14784/50000 (29%)]\tLoss: 0.157868\tError: 6.250000\n",
            "Train Epoch: 23.30 [14848/50000 (30%)]\tLoss: 0.228989\tError: 9.375000\n",
            "Train Epoch: 23.30 [14912/50000 (30%)]\tLoss: 0.407645\tError: 15.625000\n",
            "Train Epoch: 23.30 [14976/50000 (30%)]\tLoss: 0.425012\tError: 7.812500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6d1ccd5e62ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;31m# if __name__=='__main__':\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;31m# device = torch.device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-6d1ccd5e62ea>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# adjust_opt('sgd', optimizer, epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# torch.save(net, os.path.join(args.save, 'latest.pth'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-6d1ccd5e62ea>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, net, trainLoader, optimizer, device)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# make_graph.save('/tmp/t.dot', loss.creator); assert(False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mnProcessed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get the index of the max log-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBK2ETi4J8Qn",
        "colab_type": "code",
        "outputId": "3821061f-62d3-49fe-e1f8-9bb2932963a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jun 14 15:11:54 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   74C    P0    72W / 149W |  11437MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqSQLOphZ0mp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUkscdY1X_at",
        "colab_type": "code",
        "outputId": "7f07ad63-fcde-4362-d2f9-3bcbb979aae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "!ps"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    PID TTY          TIME CMD\n",
            "      1 ?        00:00:00 run.sh\n",
            "      9 ?        00:00:02 node\n",
            "     19 ?        00:00:04 jupyter-noteboo\n",
            "    110 ?        00:00:00 tail\n",
            "    119 ?        00:04:35 /usr/bin/python\n",
            "    506 ?        00:00:00 ps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M99VjJCjZPkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! kill 19"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sFd2fDvZUBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}